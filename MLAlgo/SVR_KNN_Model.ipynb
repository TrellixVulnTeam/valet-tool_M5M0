{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for converting series data to a supervised data of format, t-1, t, t+1\n",
    "## Basically feeding in the (t-1)th data to predict the t data\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_timestamp(dataset):\n",
    "    temp_time = [datetime.strptime(d, \"%Y-%m-%d %H:%M:%S.%f\") for d in dataset[\"timestamp\"]]\n",
    "    dataset[\"hours\"] = [dh.hour for dh in temp_time]\n",
    "    dataset[\"minutes\"] = [dm.minute for dm in temp_time]\n",
    "    dataset[\"seconds\"] = [ds.second for ds in temp_time]\n",
    "    \n",
    "    '''\n",
    "    for index in range(0, len(dataset[\"time\"])):\n",
    "        time_val = dataset[\"time\"].iloc[index]\n",
    "        dataset[\"hours\"] = time_val.hour\n",
    "        dataset[\"minutes\"] = time_val.minute\n",
    "        dataset[\"seconds\"] = time_val.second\n",
    "    '''\n",
    "    \n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data_arr):\n",
    "    data_max = data_arr.max()\n",
    "    data_min = data_arr.min()\n",
    "    \n",
    "    data_arr = (data_arr - data_min)/(data_max - data_min)\n",
    "    \n",
    "    return data_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          hours   minutes   seconds   latency      cost  reliability\n",
      "0      0.272917  0.579948  0.767578  0.015102  0.193359            1\n",
      "1      0.243969  0.579427  0.777652  0.015117  0.310547            1\n",
      "2      0.241814  0.589423  0.770783  0.015297  0.169922            1\n",
      "3      0.322198  0.865908  0.382611  0.014803  0.191406            1\n",
      "4      0.296806  0.871867  0.389557  0.014817  0.167969            1\n",
      "...         ...       ...       ...       ...       ...          ...\n",
      "12152  0.080750  0.452201  0.888253  0.016195  0.175781            1\n",
      "12153  0.078490  0.439544  0.894785  0.016146  0.181641            1\n",
      "12154  0.115011  0.667063  0.736070  0.016246  0.166016            1\n",
      "12155  0.152570  0.976445  0.152570  0.016016  0.167969            1\n",
      "12156  0.109817  0.702830  0.702830  0.015651  0.173828            1\n",
      "\n",
      "[12157 rows x 6 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nvalidation[\"hours\"] = normalize(validation[\"hours\"].values)\\nvalidation[\"minutes\"] = normalize(validation[\"minutes\"].values)\\nvalidation[\"seconds\"] = normalize(validation[\"seconds\"].values)\\n\\n\\n'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_arr = [\"timestamp\", \"time_since_last_recording\", \"ping_timestamp\", \"time_since_last_ping\", \"ping_success\",\"ping_time\"]\n",
    "# load dataset\n",
    "dataset = read_csv('../parse_tactics/normalized_tva_server_1_tactic_1_train.csv')\n",
    "dataset = break_timestamp(dataset)\n",
    "dataset = dataset.drop(columns=col_arr)#[\"ID\"])\n",
    "dataset = dataset[[\"hours\",\"minutes\",\"seconds\",\"latency\",\"cost\",\"reliability\"]]\n",
    "norm_scaler = Normalizer().fit(dataset.iloc[:,0:3])\n",
    "dataset.loc[:,0:3] = norm_scaler.transform(dataset.iloc[:,0:3])\n",
    "values = dataset\n",
    "'''\n",
    "dataset[\"hours\"] = normalize(dataset[\"hours\"].values)\n",
    "dataset[\"minutes\"] = normalize(dataset[\"minutes\"].values)\n",
    "dataset[\"seconds\"] = normalize(dataset[\"seconds\"].values)\n",
    "'''\n",
    "\n",
    "\n",
    "#values = dataset.values\n",
    "print(dataset)\n",
    "## Load Validation\n",
    "validation = read_csv('../parse_tactics/normalized_tva_server_1_tactic_1_test.csv')\n",
    "validation = break_timestamp(validation)\n",
    "validation = validation.drop(columns=col_arr)#[\"ID\"])\n",
    "validation = validation[[\"hours\",\"minutes\",\"seconds\",\"latency\",\"cost\",\"reliability\"]]\n",
    "validation.loc[:,0:3] = norm_scaler.transform(validation.iloc[:,0:3])\n",
    "values_validation = validation\n",
    "\n",
    "'''\n",
    "validation[\"hours\"] = normalize(validation[\"hours\"].values)\n",
    "validation[\"minutes\"] = normalize(validation[\"minutes\"].values)\n",
    "validation[\"seconds\"] = normalize(validation[\"seconds\"].values)\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   var1(t-1)  var2(t-1)  var3(t-1)  var4(t-1)  var5(t-1)  var6(t-1)   var4(t)  \\\n",
      "1   0.272917   0.579948   0.767578   0.015102   0.193359        1.0  0.015117   \n",
      "2   0.243969   0.579427   0.777652   0.015117   0.310547        1.0  0.015297   \n",
      "3   0.241814   0.589423   0.770783   0.015297   0.169922        1.0  0.014803   \n",
      "\n",
      "    var5(t)  var6(t)  \n",
      "1  0.310547        1  \n",
      "2  0.169922        1  \n",
      "3  0.191406        1  \n"
     ]
    }
   ],
   "source": [
    "## Calling the function to do the preprocessing the data and removing unwanted columns\n",
    "\n",
    "\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(values, 1, 1)\n",
    "reframed_validation = series_to_supervised(values_validation, 1, 1)\n",
    "# drop columns we don't want to predict\n",
    "reframed.drop(reframed.columns[[6,7,8]], axis=1, inplace=True)\n",
    "reframed_validation.drop(reframed_validation.columns[[6,7,8]], axis=1, inplace=True)\n",
    "print(reframed.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2604, 1, 6) (2604, 3) (12156, 1, 6) (12156, 3)\n"
     ]
    }
   ],
   "source": [
    "## Splitting the data into training and validation sets\n",
    "\n",
    "\n",
    "test = reframed.values\n",
    "train = reframed_validation.values\n",
    "# split into input and outputs\n",
    "train_X, train_y = train[:, :-3], train[:,-3:]\n",
    "test_X, test_y = test[:, :-3], test[:,-3:]\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten input\n",
    "n_input = train_X.shape[1] * train_X.shape[2]\n",
    "X = train_X.reshape((train_X.shape[0], n_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2605, 9)\n",
      "(2605, 6) (2605, 3)\n"
     ]
    }
   ],
   "source": [
    "### Doing the same process for testing dataset\n",
    "\n",
    "test_dataset = read_csv('../parse_tactics/normalized_tva_server_1_tactic_1_validation.csv')\n",
    "test_dataset = break_timestamp(test_dataset)\n",
    "test_dataset = test_dataset.drop(columns=col_arr)#[\"ID\"])\n",
    "test_dataset = test_dataset[[\"hours\",\"minutes\",\"seconds\",\"latency\",\"cost\",\"reliability\"]]\n",
    "test_dataset.iloc[:,0:3] = norm_scaler.transform(test_dataset.iloc[:,0:3])\n",
    "test_values = test_dataset#.values\n",
    "reframed_test = series_to_supervised(test_dataset, 1, 1)\n",
    "reframed_test.drop(reframed_test.columns[[6,7,8]], axis=1, inplace=True)\n",
    "print(reframed_test.shape)\n",
    "testset = reframed_test.values\n",
    "testset_X, testset_y = testset[:, :-3], testset[:,-3:]\n",
    "print(testset_X.shape, testset_y.shape)\n",
    "testdataReshaped = testset_X.reshape((testset_X.shape[0], 1, testset_X.shape[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR algorithm with RBF kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nd7896/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/home/nd7896/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/home/nd7896/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "regressor = SVR(kernel='rbf')\n",
    "# flatten input\n",
    "n_input = testdataReshaped.shape[1] * testdataReshaped.shape[2]\n",
    "X2 = testdataReshaped.reshape((testdataReshaped.shape[0], n_input))\n",
    "regr = MultiOutputRegressor(regressor)\n",
    "\n",
    "regr.fit(X,train_y)\n",
    "out= regr.predict(X2)\n",
    "\n",
    "rmse = sqrt(mean_squared_error(out,testset_y))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR with Linear kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.062\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "regressor = SVR(kernel='linear')\n",
    "# flatten input\n",
    "n_input = testdataReshaped.shape[1] * testdataReshaped.shape[2]\n",
    "X2 = testdataReshaped.reshape((testdataReshaped.shape[0], n_input))\n",
    "regr = MultiOutputRegressor(regressor)\n",
    "\n",
    "regr.fit(X,train_y)\n",
    "out= regr.predict(X2)\n",
    "\n",
    "rmse = sqrt(mean_squared_error(out,testset_y))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.052\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "knn = KNeighborsRegressor()\n",
    "regr_knn = MultiOutputRegressor(knn)\n",
    "\n",
    "# flatten input\n",
    "n_input = testdataReshaped.shape[1] * testdataReshaped.shape[2]\n",
    "X2 = testdataReshaped.reshape((testdataReshaped.shape[0], n_input))\n",
    "\n",
    "regr_knn.fit(X,train_y)\n",
    "regr_knn.predict(testset_X)\n",
    "out= regr_knn.predict(X2)\n",
    "\n",
    "rmse = sqrt(mean_squared_error(out,testset_y))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feeding the test dataset for predictions\n",
    "import pandas as pd\n",
    "\n",
    "dataset = pd.DataFrame({'predicted_Latency': out[:, 0], 'predicted_Cost': out[:, 1],\n",
    "                       'predicted_Reliability': out[:, 2]})\n",
    "dataset['predicted_Reliability'].loc[dataset['predicted_Reliability'] >0.5] = 1\n",
    "dataset['predicted_Reliability'].loc[dataset['predicted_Reliability'] <.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [pd.DataFrame(data=test_dataset), pd.DataFrame(data=dataset)]\n",
    "result = pd.concat(frames,axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      hours   minutes   seconds   latency      cost  reliability  \\\n",
      "0  0.242140  0.282497  0.928204  0.017155  0.160156            1   \n",
      "1  0.174004  0.232006  0.957024  0.014948  0.167969            1   \n",
      "2  0.130158  0.173544  0.976187  0.015292  0.164062            1   \n",
      "3  0.134670  0.224450  0.965135  0.016283  0.169922            1   \n",
      "4  0.131812  0.219687  0.966625  0.015571  0.173828            1   \n",
      "5  0.158666  0.343776  0.925550  0.015474  0.171875            1   \n",
      "6  0.354787  0.886969  0.295656  0.016747  0.175781            1   \n",
      "7  0.099231  0.264616  0.959235  0.015070  0.162109            1   \n",
      "8  0.205316  0.581728  0.787044  0.014855  0.164062            1   \n",
      "9  0.235521  0.706562  0.667308  0.016296  0.167969            1   \n",
      "\n",
      "   predicted_Latency  predicted_Cost  predicted_Reliability  \n",
      "0           0.015661        0.175000                    1.0  \n",
      "1           0.014950        0.171875                    1.0  \n",
      "2           0.015437        0.170703                    1.0  \n",
      "3           0.015296        0.174219                    1.0  \n",
      "4           0.015384        0.178125                    1.0  \n",
      "5           0.015462        0.175391                    1.0  \n",
      "6           0.015369        0.173828                    1.0  \n",
      "7           0.015441        0.176172                    1.0  \n",
      "8           0.015657        0.175391                    1.0  \n",
      "9           0.015560        0.178125                    1.0  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(result.head(10))\n",
    "result.to_csv('../Prediction_Files/predictions_SVR_RBF_Server_1_Tactic_1.csv', sep=',', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
